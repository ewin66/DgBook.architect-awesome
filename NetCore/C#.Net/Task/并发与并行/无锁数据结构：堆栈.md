## Lock-free data structures: the stack

######                published:                Thu, 27-Oct-2005                 |                 updated: Sat, 18-Apr-2009             

​                 ![Helmsley Castle, N. Yorks](assets/Helmsley_castle.gif)               

 **Notes**: Several people have emailed me to say this code suffers from the [ABA problem](https://secondboyet.com/Articles/ABAProblem.html).  It doesn't, but only because it's written in C# and implictly uses the  .NET garbage collector. If you just, quote, convert it to C++ or Delphi,  unquote, it will not work and will suffer from some form of ABA. See [here](http://blog.boyet.com/blog/blog/someone-is-wrong-on-the-internet-mdash-the-lock-free-stack-edition/) for details. 

 \------ 

 It's a fact that lock-based concurrency doesn't scale: only one thread at a time can execute the thread-safe code that's being protected by a lock or a mutex. The more threads there are accessing the lock, the more will be just sitting there waiting. (Actually "waiting" implies that they're awake: in fact, the operating system will have put them to sleep and will awaken them one at a time to acquire the lock.) 

 In pseudo-code, we have for a thread: 

```
do forever
  acquire mutex (or other lock) to resource
  do work on protected resource 
  release mutex
  maybe do other work not requiring resource
end do
```

 Meanwhile, the other threads may be doing other work, but eventually they grind to a halt, all stuck waiting for the lock to be released. And when that happens only one thread can be woken up to acquire the lock. We have the effect of a hourglass where only one grain of sand can get through the restriction at any one time, and grains that did get through can make their way to the top again. The more grains of sand the bigger the wait. Similarly the more threads using the protected resource the longer they'll have to wait proportionally to the amount of work they do.  

 It's worse as well if the machine being used has two or more CPUs, using a lock-based approach means in effect that you can't really take advantage of all CPUs. And more systems are being sold that have two CPUs on the same die, or that use hyperthreading to mimic two CPUs.  

 That's why an awful lot of work has been done over the past 10 or 15 years on lock-free or non-blocking data structures. There is a precise meaning to the term lock-free by the way: A data structure is lock-free  if it guarantees that after a finite number of steps of any operation on the data structure, some operation completes. 

 Lock-free data structures are usually implemented around some retry logic (a simple loop) as well as a strong synchronization primitive. The most widely-used such primitive used in these data structures is known as Compare-And-Swap (CAS). (A synchronization primitive is an atomic operation that cannot be interrupted and is usually implemented in hardware.) Windows (and therefore the .NET Framework) don't have CAS, but it can easily be implemented in software using another primitive that they do have.  

 Let's implement a lock-free stack using a linked list as the underlying data structure (you'll see why in a moment) to explore lock-free programming. 

 There are two operations on a stack: `Push` adds an item to the top of the stack and `Pop` removes the top item. It's extremely easy to implement such a stack in a single threaded environment. 

```
  public class SimpleStack<T> {

    private class Node<V> {
      public Node<V> Next;
      public V Item;
    }

    private Node<T> head;

    public SimpleStack() {
      head = new Node<T>();
    }

    public void Push(T item) {
      Node<T> node = new Node<T>();
      node.Item = item;
      node.Next = head.Next;
      head.Next = node;
    }

    public T Pop() {
      Node<T> node = head.Next;
      if (node == null)
        return default(T);
      head.Next = node.Next;
      return node.Item;
    }
  }
```

 This implementation uses a dummy node for `head`, off from which we hang the rest of the stack. This makes it easier to code the `Push()` method: we don't have to check for a null `head`. 

 Anyway, let's analyze `Push()` from a multithreaded viewpoint now. Getting a new node doesn't affect the stack object at all: `node` is just a local variable. Admittedly we are using a global service (the heap manager) and that may be affected by other threads. (Hold that thought as we continue.) The next line is all about local variables and can't be affected by other threads. 

 The next line is more interesting. The `head` variable is a field of the (shared) object, but it's never changed. We get the value of its `Next` field and store that in a local variable (actually a field of a local variable, which amounts to the same thing). You may not know this, but .NET guarantees that reading `head.Next` is an atomic operation. In other words, no other thread can jump in half way through us reading `head.Next` and change its value so that we somehow get half of the old value and half of the new one.  

 The next line is the horrific one, in a multithreaded sense. Just before this line executes we have made sure that our node points to the node currently pointed to by `head`. But, and its a very big but, any other thread can come along and do a `Push()` (`head` now points to another new node) or a `Pop()` (`head` now points to the next node along) before our next line executes. If this does happen, we'll trash the linkage and the stack. This is why we usually add a lock to protect our access to the head node: 

```
  public void Push(T item) {
    Node<T> node = new Node<T>();
    node.Item = item;
    lock (head) {
      node.Next = head.Next;
      head.Next = node;
    }
  }
```

 Instead of doing this, we're going to be clever and use the `CAS` function. In C# the `CAS` function would look a little like this: 

```
static bool CAS(ref object destination, object currentValue, object newValue) {
  if (destination == currentValue) {
    destination = newValue;
    return true;
  }
  return false;
}
```

 But of course the big problem is that, as written, it's not atomic. Any old thread could mess us up in the middle. In some operating systems or on some hardware there is a routine that is guaranteed to do this compare and swap atomically, but Windows doesn't have it. So we use another: `Interlocked.CompareExchange`. This method compares destination to `currentValue`, and if they're equal sets destination to `newValue` and returns `currentValue`. If they aren't equal it just returns the value of destination. And all this is done in an atomic fashion: no other thread can interrupt it. (In fact, it's done in hardware with the `CMPXCH` assembly instruction.)  

 Since we're in C# generics-land, here's the one I actually wrote: 

```
  private static bool CAS(
    ref Node<T> location, Node<T> comparand, Node<T> newValue) {
    return 
      comparand == Interlocked.CompareExchange<Node<T>>(
                     ref location, newValue, comparand);
  }
```

 Now we can use this method with some retry logic to effect linking the new node without blocking other threads. Are you ready? 

```
  public void Push(T item) {
    Node<T> node = new Node<T>();
    node.Item = item;
    do {
      node.Next = head.Next;
    } while (!CAS(ref head.Next, node.Next, node));
  }
```

   In other words, we set our node's `Next` value to `head.Next`, and try and set the `head.Next` value to our new node provided that `head.Next` hasn't changed in the meantime. If it has we just try again. And again. Eventually we shall succeed. Or rather, we can't guarantee that a particular thread may not succeed for a very long time, rather that there will always be a particular thread that will be moving forward at some particular time. 

 If you think that this is not good enough (this loop not being guaranteed to exit), consider this: in a normal locking situation, it's not guaranteed that a particular thread will not wait for a very long time at a lock. The operating system makes no guarantees about how long a particular thread waits, just that only one thread will be woken up to acquire the lock once the current thread releases it. 

 The pattern shown here is essentially how you do lock-free programming: read the current value from the variable, try and set the new value provided that the variable has not been changed. Repeat and rinse until we're successful. 

 So, now the Pop() method looks like this: 

```
  public T Pop() {
    Node<T> node;
    do {
      node = head.Next;
      if (node == null)
        return default(T);
    } while (!CAS(ref head.Next, node, node.Next));
    return node.Item;
  }
```

 Of course, we shouldn't take my word for it that this contraption of bizarre code works, we have to try this out on a dual (or more) processor machine. (Big tip for multithreaded development: always use a machine with more than one processor to test your multithreaded code. It'll fail faster if there's a bug.) 

 So I wrote this juicy little application: it pushes 10000 integers onto a stack in one thread and pops them all from anotherat the same time. Since I have a dual processor machine here, I figured that would be the best test to do.  

 The first thread just pushes the integers 1 to 10000 onto the stack. The second thread pops them off and keeps tallies of what it's seen to make sure that it pops off exactly one of each value and there are exactly 10000 of them. (Note: the way I wrote the `Pop()` method is to return the default value for the instantiating type if there was nothing on the stack. For an `int`, the default value is 0, which is why I don't use 0 as a value to push on the stack.) 

```
  class Program {

    public const int topValue = 10000;

    class PusherEngine {
      private SimpleStack<int> stack;
      public PusherEngine(SimpleStack<int> stack) {
        this.stack = stack;
      }
      public void Execute() {
        for (int i = 1; i <= topValue; i++)
        {
          stack.Push(i);
        }
      }
    }

    class PopperEngine {
      private SimpleStack<int> stack;
      private Thread pusherThread;

      public PopperEngine(SimpleStack<int> stack, Thread pusherThread) {
        this.stack = stack;
        this.pusherThread = pusherThread;
      }

      public void Execute() {
        bool[] poppedValues = new bool[topValue + 1];
        int poppedInt;

        do {
          poppedInt = stack.Pop();
          if (poppedInt != 0) {
            if (poppedValues[poppedInt])
              Console.WriteLine(string.Format("{0} has been popped before!", poppedInt));
            poppedValues[poppedInt] = true;
          }
        } while (poppedInt != topValue);

        pusherThread.Join();
        Console.WriteLine("pusher now finished");

        poppedInt = stack.Pop();
        while (poppedInt != 0) {
          if (poppedValues[poppedInt])
            Console.WriteLine(string.Format("{0} has been popped before!", poppedInt));
          poppedValues[poppedInt] = true;
          poppedInt = stack.Pop();
        }

        Console.WriteLine("checking output");
        for (int i = 1; i <= topValue; i++) {
          if (!poppedValues[i])
            Console.Write(string.Format("{0} is missing", poppedValues[i]));
        }
      }
    }

    static void Main(string[] args) {

      Console.WriteLine("create the shared stack");
      SimpleStack<int> stack = new SimpleStack<int>();

      Console.WriteLine("create the threads");
      PusherEngine pusher = new PusherEngine(stack);
      Thread pusherThread = new Thread(new ThreadStart(pusher.Execute));
      PopperEngine popper = new PopperEngine(stack, pusherThread);
      Thread popperThread = new Thread(new ThreadStart(popper.Execute));

      Console.WriteLine("start the threads");
      pusherThread.Start();
      popperThread.Start();

      popperThread.Join();

      Console.WriteLine("Done");
      Console.ReadLine();
    }
  }
```

 Because of the amount of code that gets executed in the popper thread, the pusher thread will usually finish pushing stuff much earlier than the popper thread can pop it off.  

 So what about the couple of items I asked you to bear in mind earlier on? First, we used a linked list for the data structure in order that we didn't have to pre-size the stack to use an array (for example). The big issue here would be how to grow the array when the stack filled. It's difficult without locking the array. Say we had a boolean value that we could test to see if if was safe to use the internal array. The issue is that if we detect true (it's OK to use the internal array) then another thread might creep in and clear the flag and grow the array before we had a chance to use it (so we may end up altering the array that was being thrown away).  

 Second, the heap manager is going to have to ensure multithreaded access to allow many threads to allocate memory at once and probably uses a locking methodology. So although we now have a super-cool  lock-free stack, we're using a locking heap manager inside. There's not a lot we can do about the garbage collector, but at least that's an equal-opportunity system: all .NET threads are going to be suspended during a collection. 

 In my [article on implementing a simple priority queue with queues](https://secondboyet.com/Articles/PriorityQueueLimited.html), I talked about making it lock-free. Well, this article goes a little way towards that goal ("Uh, this is all about a stack, dummy, where's the queue? Let alone where's the priority queue?"), but we've a way to go yet. See you next time, when  [we'll talk about the lock-free queue](https://secondboyet.com/Articles/LockfreeQueue.html). 

###	Here's a bunch of links for you to tie all this lock-free stuff up. 

- [Lock-free data structures: the stack](https://secondboyet.com/Articles/LockfreeStack.html)   
- [Lock-free data structures: the queue](https://secondboyet.com/Articles/LockfreeQueue.html)   
- [Lock-free data structures: the free list](https://secondboyet.com/Articles/LockfreeFreeList.html)   
- [Lock-free data structures: the limited-priority queue](https://secondboyet.com/Articles/LockFreeLimitedPriorityQ.html)   
- [Lock-free data structures: redux](https://secondboyet.com/Articles/LockFreeRedux.html)   
- [Lock-free data structures: the code](https://secondboyet.com/Code/LockFreeCode.zip) 







## 无锁数据结构：堆栈

###### 出版：清华，2005年10月27日-2005年更新：星期六，2009年4月18日

![Helmsley Castle, N. Yorks](assets/Helmsley_castle-1560391294789.gif)

注记：有几个人给我发电子邮件说这段代码受到了[ABA问题](https://secondboyet.com/Articles/ABAProblem.html)..它没有，只是因为它是用C#编写的，并且实际上使用了.NET垃圾收集器。如果你只是，报价，转换成C+或德尔菲，取消报价，它将无法工作，并将遭受某种形式的ABA。看见[这里](http://blog.boyet.com/blog/blog/someone-is-wrong-on-the-internet-mdash-the-lock-free-stack-edition/)关于细节。

\------

基于锁的并发不能扩展：一次只有一个线程可以执行由锁或互斥保护的线程安全代码。访问锁的线程越多，就会有越多的线程坐在那里等待。(实际上，“等待”意味着它们是清醒的：实际上，操作系统会让它们进入睡眠状态，并每次唤醒它们一个来获取锁。)

在伪代码中，我们有一个线程：

```
do forever
  acquire mutex (or other lock) to resource
  do work on protected resource 
  release mutex
  maybe do other work not requiring resource
end do
```

与此同时，其他线程可能正在做其他工作，但最终它们会停止工作，所有线程都被困在等待锁被释放的状态。当这种情况发生时，只能唤醒一个线程来获取锁。我们的效果就像沙漏，在任何时候只有一粒沙子可以通过限制，而通过沙漏的沙粒可以再次到达顶端。沙粒越多，等待的时间就越大。类似地，使用受保护资源的线程越多，它们根据工作量的比例等待的时间就越长。

更糟糕的是，如果所使用的机器有两个或多个CPU，使用基于锁的方法实际上意味着不能真正利用所有CPU。还有更多的系统在同一芯片上有两个CPU，或者使用超线程模拟两个CPU。

这就是为什么在过去的10到15年里，在无锁或非阻塞的数据结构方面做了大量的工作。顺便说一句，“无锁”一词有一个确切的含义：数据结构是无锁的，如果它保证在数据结构上的任何操作经过有限的步骤之后，一些操作就完成了。

无锁数据结构通常是围绕一些重试逻辑(一个简单的循环)以及一个强同步原语来实现的。在这些数据结构中使用最广泛的这种原语称为比较交换(CAS)。(同步原语是一个原子操作，不能中断，通常在硬件中实现。)Windows(因此也是.NET Framework)没有CAS，但是它可以很容易地在软件中使用它们拥有的另一个原语来实现。

让我们使用链接列表作为底层数据结构来实现一个无锁堆栈(稍后您将看到原因)来探索无锁编程。

堆栈上有两个操作：`Push`将项添加到堆栈顶部，则`Pop`移除顶部项。在单个线程环境中实现这样的堆栈非常容易。

```
  public class SimpleStack<T> {

    private class Node<V> {
      public Node<V> Next;
      public V Item;
    }

    private Node<T> head;

    public SimpleStack() {
      head = new Node<T>();
    }

    public void Push(T item) {
      Node<T> node = new Node<T>();
      node.Item = item;
      node.Next = head.Next;
      head.Next = node;
    }

    public T Pop() {
      Node<T> node = head.Next;
      if (node == null)
        return default(T);
      head.Next = node.Next;
      return node.Item;
    }
  }
```

此实现使用一个虚拟节点`head`我们把剩下的东西挂在上面。这使得编写`Push()`方法：我们不需要检查空值`head`.

不管怎样，让我们分析一下`Push()`从多线程的角度来看。获取一个新节点根本不影响堆栈对象：`node`只是局部变量。诚然，我们使用的是全局服务(堆管理器)，这可能会受到其他线程的影响。(在我们继续的时候保持这种想法。)下一行都是关于局部变量的，不会受到其他线程的影响。

下一行更有趣。这个`head`变量是(Shared)对象的一个字段，但它从未更改过。我们得到它的价值`Next`字段并将其存储在局部变量中(实际上是局部变量的字段，相当于相同的内容)。您可能不知道这一点，但.NET保证读取`head.Next`是原子操作。换句话说，在我们阅读的一半时间里，没有其他线程可以跳过。`head.Next`改变它的价值，让我们得到一半的旧值和一半的新值。

下一行是可怕的，在多线程意义上。在执行此行之前，我们已经确保我们的节点指向当前指向的节点。`head`..但是，这是一个很大的但是，任何其他线程都可以来做一个`Push()` (`head`现在指向另一个新节点)或`Pop()` (`head`现在指向下一行执行之前的下一个节点)。如果发生这种情况，我们将破坏链接和堆栈。这就是为什么我们通常添加一个锁来保护对Head节点的访问：

```
  public void Push(T item) {
    Node<T> node = new Node<T>();
    node.Item = item;
    lock (head) {
      node.Next = head.Next;
      head.Next = node;
    }
  }
```

我们将不再这样做，而是聪明地使用`CAS`功能。在C#中`CAS`函数看起来有点像这样：

```
static bool CAS(ref object destination, object currentValue, object newValue) {
  if (destination == currentValue) {
    destination = newValue;
    return true;
  }
  return false;
}
```

但当然，最大的问题是，正如所写的，它不是原子的。任何旧线都可能使我们陷入困境。在某些操作系统或某些硬件上，有一个例程可以保证通过原子方式进行比较和交换，但Windows没有。所以我们用另一个：`Interlocked.CompareExchange`..此方法将目标与`currentValue`，如果它们是相等的，则将目的地设置为`newValue`和回报`currentValue`..如果它们不相等，它只返回目的地的值。所有这些都是以原子的方式完成的：没有其他线程可以中断它。(实际上，它是在硬件中使用`CMPXCH`装配指令)

既然我们在C#泛型领域，下面是我写的那篇文章：

```
  private static bool CAS(
    ref Node<T> location, Node<T> comparand, Node<T> newValue) {
    return 
      comparand == Interlocked.CompareExchange<Node<T>>(
                     ref location, newValue, comparand);
  }
```

现在，我们可以使用这个方法和一些重试逻辑来实现新节点的链接，而不会阻塞其他线程。准备好了吗？

```
  public void Push(T item) {
    Node<T> node = new Node<T>();
    node.Item = item;
    do {
      node.Next = head.Next;
    } while (!CAS(ref head.Next, node.Next, node));
  }
```

换句话说，我们设置节点的`Next`价值`head.Next`，并尝试设置`head.Next`值到我们的新节点，条件是`head.Next`这段时间没变。如果有，我们就再试一次。一次又一次。我们最终会成功的。或者说，我们不能保证一个特定的线程在很长一段时间内不会成功，而是总是会有一个特定的线程在某个特定的时间继续前进。

如果您认为这不够好(此循环不能保证退出)，请考虑以下问题：在正常锁定情况下，不能保证某个特定线程在锁定时不会等待很长时间。操作系统并不保证某个特定线程等待多长时间，只是在当前线程释放锁后，只会唤醒一个线程来获取锁。

这里显示的模式基本上是您如何进行无锁编程：从变量读取当前值，尝试并设置新值，前提是变量尚未更改。重复并冲洗直到我们成功。

现在，Pop()方法如下所示：

```
  public T Pop() {
    Node<T> node;
    do {
      node = head.Next;
      if (node == null)
        return default(T);
    } while (!CAS(ref head.Next, node, node.Next));
    return node.Item;
  }
```

当然，我们不应该相信我的话，这个奇怪的代码装置可以工作，我们必须在一台双(或更多)处理器机器上尝试这一点。(多线程开发的大提示：总是使用一台有多个处理器的机器来测试多线程代码。如果有错误，它会更快地失败。)

因此，我编写了这个多汁的小应用程序：它将10000个整数推到一个线程中的堆栈上，同时将它们从另一个线程中弹出。因为我这里有一台双处理器的机器，所以我想这将是最好的测试。

第一个线程只是将整数1推到10000到堆栈上。第二个线程会弹出它们，并记录它所看到的内容，以确保每一个值中有一个是弹出的，其中正好有10000个。(注：我写的方式`Pop()`方法是，如果堆栈中没有任何内容，则返回实例化类型的默认值。为了`int`，默认值为0，这就是为什么我不使用0作为推送堆栈的值。)

```
  class Program {

    public const int topValue = 10000;

    class PusherEngine {
      private SimpleStack<int> stack;
      public PusherEngine(SimpleStack<int> stack) {
        this.stack = stack;
      }
      public void Execute() {
        for (int i = 1; i <= topValue; i++)
        {
          stack.Push(i);
        }
      }
    }

    class PopperEngine {
      private SimpleStack<int> stack;
      private Thread pusherThread;

      public PopperEngine(SimpleStack<int> stack, Thread pusherThread) {
        this.stack = stack;
        this.pusherThread = pusherThread;
      }

      public void Execute() {
        bool[] poppedValues = new bool[topValue + 1];
        int poppedInt;

        do {
          poppedInt = stack.Pop();
          if (poppedInt != 0) {
            if (poppedValues[poppedInt])
              Console.WriteLine(string.Format("{0} has been popped before!", poppedInt));
            poppedValues[poppedInt] = true;
          }
        } while (poppedInt != topValue);

        pusherThread.Join();
        Console.WriteLine("pusher now finished");

        poppedInt = stack.Pop();
        while (poppedInt != 0) {
          if (poppedValues[poppedInt])
            Console.WriteLine(string.Format("{0} has been popped before!", poppedInt));
          poppedValues[poppedInt] = true;
          poppedInt = stack.Pop();
        }

        Console.WriteLine("checking output");
        for (int i = 1; i <= topValue; i++) {
          if (!poppedValues[i])
            Console.Write(string.Format("{0} is missing", poppedValues[i]));
        }
      }
    }

    static void Main(string[] args) {

      Console.WriteLine("create the shared stack");
      SimpleStack<int> stack = new SimpleStack<int>();

      Console.WriteLine("create the threads");
      PusherEngine pusher = new PusherEngine(stack);
      Thread pusherThread = new Thread(new ThreadStart(pusher.Execute));
      PopperEngine popper = new PopperEngine(stack, pusherThread);
      Thread popperThread = new Thread(new ThreadStart(popper.Execute));

      Console.WriteLine("start the threads");
      pusherThread.Start();
      popperThread.Start();

      popperThread.Join();

      Console.WriteLine("Done");
      Console.ReadLine();
    }
  }
```

由于在Popper线程中执行的代码数量很大，推送线程通常会比Popper线程弹出时更早完成推送。

那我刚才让你记住的几个项目呢？首先，我们为数据结构使用了一个链接列表，这样我们就不必预先调整堆栈的大小才能使用数组(例如)。这里的大问题是如何在堆栈填充时扩展数组。不锁定数组是很困难的。假设我们有一个布尔值，可以进行测试，以确定使用内部数组是否安全。问题是，如果我们检测到真(使用内部数组是可以的)，那么另一个线程可能会在我们有机会使用它之前悄悄地进入并清除该标志并增长该数组(因此，我们可能最终会更改被丢弃的数组)。

其次，堆管理器必须确保多线程访问，以便允许多个线程同时分配内存，并且可能使用锁定方法。因此，尽管我们现在有了一个超级酷的无锁堆栈，但我们在里面使用了一个锁定堆管理器。对于垃圾收集器我们无能为力，但至少这是一个机会均等的系统：所有.NET线程都将在收集过程中被挂起。

在我的[关于使用队列实现简单优先级队列的文章](https://secondboyet.com/Articles/PriorityQueueLimited.html)我说过要让它没有锁。好吧，这篇文章朝着这个目标走了一段路(“嗯，这都是一个堆栈，虚拟的，队列在哪里？更不用说优先级队列在哪里了？”)，但我们还有一段路要走。下次见[我们将讨论无锁队列](https://secondboyet.com/Articles/LockfreeQueue.html).

这里有一系列的链接，让你把所有这些无锁的东西都绑起来。